{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session = SparkSession.builder.master(\"local[*]\").appName('Models').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training data: 98550\n",
      "Size of the test data: 42237\n"
     ]
    }
   ],
   "source": [
    "# Read datasets with spark\n",
    "train_df_spark = spark_session.read.csv('../datasets/train.csv', header=True, inferSchema=True)\n",
    "test_df_spark = spark_session.read.csv('../datasets/test.csv', header=True, inferSchema=True)\n",
    "\n",
    "print(\"Size of the training data:\", train_df_spark.count())\n",
    "print(\"Size of the test data:\", test_df_spark.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+------+---------------+\n",
      "| f1|  f2|  f3|target|       features|\n",
      "+---+----+----+------+---------------+\n",
      "|0.0| 6.0|61.0|   1.0| [0.0,6.0,61.0]|\n",
      "|1.0|12.0|27.0|   0.0|[1.0,12.0,27.0]|\n",
      "|0.0|14.0|17.0|   0.0|[0.0,14.0,17.0]|\n",
      "|0.0| 6.0|24.0|   0.0| [0.0,6.0,24.0]|\n",
      "|0.0| 6.0|38.0|   0.0| [0.0,6.0,38.0]|\n",
      "+---+----+----+------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+----+----+------+--------------+\n",
      "| f1|  f2|  f3|target|      features|\n",
      "+---+----+----+------+--------------+\n",
      "|0.0| 1.0|26.0|   0.0|[0.0,1.0,26.0]|\n",
      "|0.0| 8.0|45.0|   1.0|[0.0,8.0,45.0]|\n",
      "|0.0|15.0| 1.0|   0.0|[0.0,15.0,1.0]|\n",
      "|0.0| 6.0|36.0|   0.0|[0.0,6.0,36.0]|\n",
      "|1.0| 4.0|46.0|   0.0|[1.0,4.0,46.0]|\n",
      "+---+----+----+------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Encode the features into a vector\n",
    "featureassemble = VectorAssembler(inputCols=['f1','f2','f3'], outputCol='features')\n",
    "output = featureassemble.transform(train_df_spark)\n",
    "output.show(n=5)\n",
    "\n",
    "testfeatureassemble = VectorAssembler(inputCols=['f1','f2','f3'], outputCol='features')\n",
    "testoutput = testfeatureassemble.transform(test_df_spark)\n",
    "testoutput.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+\n",
      "|       features|target|\n",
      "+---------------+------+\n",
      "| [0.0,6.0,61.0]|   1.0|\n",
      "|[1.0,12.0,27.0]|   0.0|\n",
      "|[0.0,14.0,17.0]|   0.0|\n",
      "| [0.0,6.0,24.0]|   0.0|\n",
      "| [0.0,6.0,38.0]|   0.0|\n",
      "+---------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------+------+\n",
      "|      features|target|\n",
      "+--------------+------+\n",
      "|[0.0,1.0,26.0]|   0.0|\n",
      "|[0.0,8.0,45.0]|   1.0|\n",
      "|[0.0,15.0,1.0]|   0.0|\n",
      "|[0.0,6.0,36.0]|   0.0|\n",
      "|[1.0,4.0,46.0]|   0.0|\n",
      "+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the features and the target column\n",
    "\n",
    "train = output.select('features', 'target') \n",
    "train.show(n=5)\n",
    "\n",
    "test = testoutput.select('features', 'target')\n",
    "test.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.79%\n",
      "Weighted Precision: 81.38%\n",
      "Weighted Recall: 82.79%\n",
      "F1 Score: 81.10%\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(labelCol='target').fit(train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "results = classifier.evaluate(test) \n",
    "\n",
    "# Print the accuracy, precision, recall and f1 score\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(results.predictions)\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "weightedPrecision = evaluator.evaluate(results.predictions)\n",
    "print(f\"Weighted Precision: {weightedPrecision*100:.2f}%\")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "weightedRecall = evaluator.evaluate(results.predictions)\n",
    "print(f\"Weighted Recall: {weightedRecall*100:.2f}%\")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = evaluator.evaluate(results.predictions)\n",
    "print(f\"F1 Score: {f1*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.84%\n",
      "Weighted Precision: 80.28%\n",
      "Weighted Recall: 77.84%\n",
      "F1 Score: 78.74%\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayes(labelCol='target', featuresCol='features')\n",
    "classifier = classifier.fit(train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "results = classifier.transform(test)\n",
    "\n",
    "# Print the accuracy, precision, recall and f1 score\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(results)\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "weightedPrecision = evaluator.evaluate(results)\n",
    "print(f\"Weighted Precision: {weightedPrecision*100:.2f}%\")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "weightedRecall = evaluator.evaluate(results)\n",
    "print(f\"Weighted Recall: {weightedRecall*100:.2f}%\")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = evaluator.evaluate(results)\n",
    "print(f\"F1 Score: {f1*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.87%\n",
      "Weighted Precision: 81.68%\n",
      "Weighted Recall: 82.87%\n",
      "F1 Score: 80.55%\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(labelCol='target', featuresCol='features')\n",
    "classifier = classifier.fit(train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "results = classifier.transform(test)\n",
    "\n",
    "# Print the accuracy, precision, recall and f1 score\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(results)\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "weightedPrecision = evaluator.evaluate(results)\n",
    "print(f\"Weighted Precision: {weightedPrecision*100:.2f}%\")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "weightedRecall = evaluator.evaluate(results)\n",
    "print(f\"Weighted Recall: {weightedRecall*100:.2f}%\")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = evaluator.evaluate(results)\n",
    "print(f\"F1 Score: {f1*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.83%\n",
      "Weighted Precision: 81.47%\n",
      "Weighted Recall: 82.83%\n",
      "F1 Score: 80.87%\n"
     ]
    }
   ],
   "source": [
    "classifier = LinearSVC(labelCol='target', featuresCol='features')\n",
    "classifier = classifier.fit(train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "results = classifier.transform(test)\n",
    "\n",
    "# Print the accuracy, precision, recall and f1 score\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(results)\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "weightedPrecision = evaluator.evaluate(results)\n",
    "print(f\"Weighted Precision: {weightedPrecision*100:.2f}%\")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "weightedRecall = evaluator.evaluate(results)\n",
    "print(f\"Weighted Recall: {weightedRecall*100:.2f}%\")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = evaluator.evaluate(results)\n",
    "print(f\"F1 Score: {f1*100:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier using Map-Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to RDD\n",
    "train_rdd_spark = train_df_spark.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of occurence of each class and each value of the features\n",
    "f1_map = train_rdd_spark.map(lambda x: ((x[0]), 1))\n",
    "f2_map = train_rdd_spark.map(lambda x: ((x[1]), 1))\n",
    "f3_map = train_rdd_spark.map(lambda x: ((x[2]), 1))\n",
    "target_map = train_rdd_spark.map(lambda x: (x[3], 1))\n",
    "\n",
    "# Reduce the data to count the number of each class\n",
    "f1_reduce = f1_map.reduceByKey(lambda x, y: x + y)\n",
    "f2_reduce = f2_map.reduceByKey(lambda x, y: x + y)\n",
    "f3_reduce = f3_map.reduceByKey(lambda x, y: x + y)\n",
    "target_reduce = target_map.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "\n",
    "# Map the data to count the occurence of each class with the different values of the features\n",
    "f1_target_map = train_rdd_spark.map(lambda x: ((x[0], x[3]), 1))\n",
    "f2_target_map = train_rdd_spark.map(lambda x: ((x[1], x[3]), 1))\n",
    "f3_target_map = train_rdd_spark.map(lambda x: ((x[2], x[3]), 1))\n",
    "\n",
    "# Reduce the data to count the number of each class\n",
    "f1_target_reduce = f1_target_map.reduceByKey(lambda x, y: x + y)\n",
    "f2_target_reduce = f2_target_map.reduceByKey(lambda x, y: x + y)\n",
    "f3_target_reduce = f3_target_map.reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to calculate the propability of each class for each value of the features\n",
    "\n",
    "# f1\n",
    "prop_f1_target_reduce = f1_target_reduce.map(lambda x: (x[0][0], (x[0][1], x[1]))) #Convert the schema ((f1, class), count) -> (f1, (class, count))\n",
    "prop_f1_target_reduce = prop_f1_target_reduce.join(f1_reduce) # Join the RDDs to get the total number of occurence of each f1 (f1, ((class, count), total)), that's why we needed the above step\n",
    "prop_f1_target_reduce = prop_f1_target_reduce.map(lambda x: (x[0], (x[1][0][0], x[1][0][1]), x[1][1])) # Convert the schema (f1, ((class, count), total)) -> (f1, (class, count), total)\n",
    "prop_f1_target_reduce = prop_f1_target_reduce.map(lambda x: (x[0], (x[1][0], x[1][1]/x[2])))\n",
    "prop_f1_target_reduce = prop_f1_target_reduce.groupByKey().mapValues(list)\n",
    "\n",
    "# f2\n",
    "prop_f2_target_reduce = f2_target_reduce.map(lambda x: (x[0][0], (x[0][1], x[1]))) #Convert the schema ((f2, class), count) -> (f2, (class, count))\n",
    "prop_f2_target_reduce = prop_f2_target_reduce.join(f2_reduce) # Join the RDDs to get the total number of occurence of each f2 (f2, ((class, count), total)), that's why we needed the above step\n",
    "prop_f2_target_reduce = prop_f2_target_reduce.map(lambda x: (x[0], (x[1][0][0], x[1][0][1]), x[1][1])) # Convert the schema (f2, ((class, count), total)) -> (f2, (class, count), total)\n",
    "prop_f2_target_reduce = prop_f2_target_reduce.map(lambda x: (x[0], (x[1][0], x[1][1]/x[2])))\n",
    "prop_f2_target_reduce = prop_f2_target_reduce.groupByKey().mapValues(list)\n",
    "\n",
    "\n",
    "# f3\n",
    "prop_f3_target_reduce = f3_target_reduce.map(lambda x: (x[0][0], (x[0][1], x[1]))) #Convert the schema ((f3, class), count) -> (f3, (class, count))\n",
    "prop_f3_target_reduce = prop_f3_target_reduce.join(f3_reduce) # Join the RDDs to get the total number of occurence of each f3 (f3, ((class, count), total)), that's why we needed the above step\n",
    "prop_f3_target_reduce = prop_f3_target_reduce.map(lambda x: (x[0], (x[1][0][0], x[1][0][1]), x[1][1])) # Convert the schema (f3, ((class, count), total)) -> (f3, (class, count), total)\n",
    "prop_f3_target_reduce = prop_f3_target_reduce.map(lambda x: (x[0], (x[1][0], x[1][1]/x[2])))\n",
    "prop_f3_target_reduce = prop_f3_target_reduce.groupByKey().mapValues(list)\n",
    "\n",
    "\n",
    "# Number of records\n",
    "N = train_rdd_spark.count()\n",
    "prop_target_reduce = target_reduce.map(lambda x: (x[0], x[1] / N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to convert the RDDs to dictionaries to be able to use them in the prediction function\n",
    "prop_f1_target_reduce_dict = prop_f1_target_reduce.collectAsMap()\n",
    "prop_f2_target_reduce_dict = prop_f2_target_reduce.collectAsMap()\n",
    "prop_f3_target_reduce_dict = prop_f3_target_reduce.collectAsMap()\n",
    "prop_target_reduce_dict = prop_target_reduce.collectAsMap()\n",
    "\n",
    "# Need to sort the values of the dictionaries by the class to be able to use them in the prediction function\n",
    "for key in prop_f1_target_reduce_dict:\n",
    "    prop_f1_target_reduce_dict[key].sort(key=lambda x: x[0])\n",
    "    \n",
    "for key in prop_f2_target_reduce_dict:\n",
    "    prop_f2_target_reduce_dict[key].sort(key=lambda x: x[0])\n",
    "    \n",
    "for key in prop_f3_target_reduce_dict:\n",
    "    prop_f3_target_reduce_dict[key].sort(key=lambda x: x[0])\n",
    "    \n",
    "prop_target_reduce_dict = sorted(prop_target_reduce_dict.items(), key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the target target of a pet given its features\n",
    "def predict(f1, f2, f3_):\n",
    "    # f1\n",
    "    f1_target = prop_f1_target_reduce_dict[f1] # This will return a list of tuples (class, probability)\n",
    "    f1_target = [x[1] for x in f1_target] # We only want the probabilities (the class is the index of the list)\n",
    "    # f2\n",
    "    f2_target = prop_f2_target_reduce_dict[f2]\n",
    "    f2_target = [x[1] for x in f2_target]\n",
    "    \n",
    "    # f3\n",
    "    f3_target = prop_f3_target_reduce_dict[f3_]\n",
    "    f3_target = [x[1] for x in f3_target]\n",
    "    \n",
    "    # We compute the product of the probabilities of each class given the features\n",
    "    prob = [a*b*c for a,b,c in zip(f1_target, f2_target, f3_target)]\n",
    "    \n",
    "    # We compute the argmax of the probabilities\n",
    "    prediction = prob.index(max(prob))\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.83%\n",
      "Precision: 81.79%\n",
      "Recall: 80.83%\n",
      "F1: 75.38%\n"
     ]
    }
   ],
   "source": [
    "## Now we can predict the target target of a pet given its features\n",
    "test_data = pd.read_csv(\"../datasets/test.csv\")\n",
    "\n",
    "\n",
    "# For each pet in the test set we predict the target target\n",
    "f1s = test_data[\"f1\"].tolist()\n",
    "f2s = test_data[\"f2\"].tolist()\n",
    "f3s = test_data[\"f3\"].tolist()\n",
    "y_true = test_data[\"target\"].tolist()\n",
    "\n",
    "y_pred = []\n",
    "\n",
    "for i in range(len(f1s)):\n",
    "    f1 = f1s[i]\n",
    "    f2 = f2s[i]\n",
    "    f3_ = f3s[i]\n",
    "    prediction = predict(f1, f2, f3_)\n",
    "    y_pred.append(prediction)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_true, y_pred)*100:.2f}%\")\n",
    "print(f\"Precision: {precision_score(y_true, y_pred, average='weighted')*100:.2f}%\")\n",
    "print(f\"Recall: {recall_score(y_true, y_pred, average='weighted')*100:.2f}%\")\n",
    "print(f\"F1: {f1_score(y_true, y_pred, average='weighted')*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
