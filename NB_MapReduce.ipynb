{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session = SparkSession.builder.master(\"local[*]\").config(\"spark.driver.memory\", \"15g\").appName('NB_MapReduce').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset with spark\n",
    "train_df_spark = spark_session.read.csv('train.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = train_df_spark.columns[:-1]\n",
    "output_col = train_df_spark.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------------+-----------+-----------+-----------+-----------+--------+--------+---------+------------+--------------------+\n",
      "|Rainfall|Sunshine|WindGustSpeed|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|Cloud9am|Cloud3pm|RainToday|RainTomorrow|            features|\n",
      "+--------+--------+-------------+-----------+-----------+-----------+-----------+--------+--------+---------+------------+--------------------+\n",
      "|       0|       4|           15|         48|         69|         45|         30|       6|       7|        0|           1|[0.0,4.0,15.0,48....|\n",
      "|       0|      24|           25|         33|         19|        140|        137|       7|       7|        0|           0|[0.0,24.0,25.0,33...|\n",
      "|       0|       8|           37|         55|         82|         62|         65|       7|       7|        0|           0|[0.0,8.0,37.0,55....|\n",
      "|       1|      14|           13|         55|         48|        182|        191|       4|       0|        0|           0|[1.0,14.0,13.0,55...|\n",
      "|       0|      14|            7|         87|         76|        110|         98|       4|       4|        0|           1|[0.0,14.0,7.0,87....|\n",
      "+--------+--------+-------------+-----------+-----------+-----------+-----------+--------+--------+---------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Encode the features into a vector\n",
    "featureassemble = VectorAssembler(inputCols=input_cols, outputCol='features')\n",
    "output = featureassemble.transform(train_df_spark)\n",
    "output.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to RDD\n",
    "train_rdd_spark = train_df_spark.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NB_Classifier:\n",
    "    def __init__(self, input_cols):\n",
    "        self.input_cols = input_cols\n",
    "\n",
    "    def fit(self, train_rdd_spark):\n",
    "        # Find the probability of each class in the dataset\n",
    "        target_map = train_rdd_spark.map(lambda x: (x[len(self.input_cols)], 1))\n",
    "        target_reduce = target_map.reduceByKey(lambda x, y: x + y)\n",
    "        num_records = train_rdd_spark.count()\n",
    "        probability_target_reduce = target_reduce.map(lambda x: (x[0], x[1] / num_records))\n",
    "        self.probability_target_reduce_dict = probability_target_reduce.collectAsMap()\n",
    "        self.probability_target_reduce_dict = sorted(self.probability_target_reduce_dict.items(), key=lambda x: x[0])\n",
    "\n",
    "        f_map = []\n",
    "        for i in range(len(self.input_cols)):\n",
    "            f_map.append(train_rdd_spark.map(lambda x: ((x[i]), 1)))\n",
    "\n",
    "        f_reduce = []\n",
    "        for i in range(len(self.input_cols)):\n",
    "            f_reduce.append(f_map[i].reduceByKey(lambda x, y: x + y))\n",
    "\n",
    "        f_target_map = []\n",
    "        for i in range(len(self.input_cols)):\n",
    "            f_target_map.append(train_rdd_spark.map(lambda x: ((x[i], x[len(self.input_cols)]), 1)))\n",
    "\n",
    "        f_target_reduce = []\n",
    "        for i in range(len(self.input_cols)):\n",
    "            f_target_reduce.append(f_target_map[i].reduceByKey(lambda x, y: x + y))\n",
    "\n",
    "        probability_f_target_reduce = []\n",
    "        for i in range(len(self.input_cols)):\n",
    "            probability_f_target_reduce.append(f_target_reduce[i].map(lambda x: (x[0][0], (x[0][1], x[1]))))\n",
    "            probability_f_target_reduce[i] = probability_f_target_reduce[i].join(f_reduce[i])\n",
    "            probability_f_target_reduce[i] = probability_f_target_reduce[i].map(lambda x: (x[0], (x[1][0][0], x[1][0][1]), x[1][1]))\n",
    "            probability_f_target_reduce[i] = probability_f_target_reduce[i].map(lambda x: (x[0], (x[1][0], x[1][1] / x[2])))\n",
    "            probability_f_target_reduce[i] = probability_f_target_reduce[i].groupByKey().mapValues(list)\n",
    "\n",
    "        self.probability_f_target_reduce_dict = []\n",
    "        for i in range(len(self.input_cols)):\n",
    "            self.probability_f_target_reduce_dict.append(probability_f_target_reduce[i].collectAsMap())\n",
    "\n",
    "        for i in range(len(self.input_cols)):\n",
    "            for key in self.probability_f_target_reduce_dict[i]:\n",
    "                self.probability_f_target_reduce_dict[i][key].sort(key=lambda x: x[0])\n",
    "            \n",
    "    def predict(self, features):\n",
    "        f_target = []\n",
    "        for i in range(len(self.input_cols)):\n",
    "            if features[i] in self.probability_f_target_reduce_dict[i]:\n",
    "                f_target.append(self.probability_f_target_reduce_dict[i][features[i]])\n",
    "                f_target[i] = [x[1] for x in f_target[i]]\n",
    "                if len(f_target[i]) < len(self.probability_target_reduce_dict):\n",
    "                    if f_target[i][0] == 0:\n",
    "                        f_target[i].insert(1, 0)\n",
    "                    else:\n",
    "                        f_target[i].insert(0, 0)\n",
    "            else:\n",
    "                f_target.append([0] * len(self.probability_target_reduce_dict))\n",
    "                \n",
    "        prob = [1] * len(f_target[0])\n",
    "\n",
    "        for j in range(len(f_target[0])):\n",
    "            for i in range(len(self.input_cols)):\n",
    "                prob[j] *= f_target[i][j]\n",
    "        \n",
    "        # Multiply by the probability of the class\n",
    "        for i in range(len(f_target[0])):\n",
    "            prob[i] *= self.probability_target_reduce_dict[i][1]\n",
    "        # Argmax\n",
    "        prediction = prob.index(max(prob))\n",
    "        \n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier with MapReduce\n",
      "Accuracy: 77.4806%\n",
      "Weighted Precision: 82.5540%\n",
      "Weighted Recall: 77.4806%\n",
      "Weighted F1-score: 67.6928%\n"
     ]
    }
   ],
   "source": [
    "classifier = NB_Classifier(input_cols)\n",
    "classifier.fit(train_rdd_spark)\n",
    "\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "test_features = []\n",
    "for i in range(len(input_cols)):\n",
    "    test_features.append(test_data[input_cols[i]].tolist())\n",
    "\n",
    "y_true = test_data[output_col].tolist()\n",
    "y_pred = []\n",
    "\n",
    "for i in range(len(test_features[0])):\n",
    "    features = []\n",
    "    for j in range(len(input_cols)):\n",
    "        features.append(test_features[j][i])\n",
    "    prediction = classifier.predict(features)\n",
    "    y_pred.append(prediction)\n",
    "\n",
    "print(\"Naive Bayes Classifier with MapReduce\")\n",
    "print(f\"Accuracy: {accuracy_score(y_true, y_pred) * 100:.4f}%\")\n",
    "print(f\"Weighted Precision: {precision_score(y_true, y_pred, average='weighted') * 100:.4f}%\")\n",
    "print(f\"Weighted Recall: {recall_score(y_true, y_pred, average='weighted') * 100:.4f}%\")\n",
    "print(f\"Weighted F1-score: {f1_score(y_true, y_pred, average='weighted') * 100:.4f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
