{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session = SparkSession.builder.master(\"local[*]\").config(\"spark.driver.memory\", \"15g\").appName('NB_MapReduce').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset with spark\n",
    "train_df_spark = spark_session.read.csv('train.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = train_df_spark.columns[:-1]\n",
    "output_col = train_df_spark.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+---+---+---+------+--------------------+\n",
      "| f1| f2| f3| f4| f5| f6| f7| f8| f9|f10|target|            features|\n",
      "+---+---+---+---+---+---+---+---+---+---+------+--------------------+\n",
      "|  0| 21| 20| 61| 51|142|141|  8|  4|  0|     0|[0.0,21.0,20.0,61...|\n",
      "|  0| 14| 15| 29| 35|164|168|  4|  4|  0|     0|[0.0,14.0,15.0,29...|\n",
      "|  1| 20|  8| 65| 59|221|225|  4|  4|  0|     0|[1.0,20.0,8.0,65....|\n",
      "|101| 14| 13| 78| 84|111|120|  4|  4|  0|     1|[101.0,14.0,13.0,...|\n",
      "|  0|  0| 11| 20| 52| 47| 66|  7|  7|  0|     0|[0.0,0.0,11.0,20....|\n",
      "+---+---+---+---+---+---+---+---+---+---+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Encode the features into a vector\n",
    "featureassemble = VectorAssembler(inputCols=input_cols, outputCol='features')\n",
    "output = featureassemble.transform(train_df_spark)\n",
    "output.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to RDD\n",
    "train_rdd_spark = train_df_spark.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of occurence of each class and each value of the features\n",
    "f_map = []\n",
    "for i in range(len(input_cols)):\n",
    "    f_map.append(train_rdd_spark.map(lambda x: ((x[i]), 1)))\n",
    "target_map = train_rdd_spark.map(lambda x: (x[len(input_cols)], 1))\n",
    "\n",
    "# Reduce the data to count the number of each class\n",
    "f_reduce = []\n",
    "for i in range(len(input_cols)):\n",
    "    f_reduce.append(f_map[i].reduceByKey(lambda x, y: x + y))\n",
    "target_reduce = target_map.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "\n",
    "# Map the data to count the occurence of each class with the different values of the features\n",
    "f_target_map = []\n",
    "for i in range(len(input_cols)):\n",
    "    f_target_map.append(train_rdd_spark.map(lambda x: ((x[i], x[len(input_cols)]), 1)))\n",
    "\n",
    "# Reduce the data to count the number of each class\n",
    "f_target_reduce = []\n",
    "for i in range(len(input_cols)):\n",
    "    f_target_reduce.append(f_target_map[i].reduceByKey(lambda x, y: x + y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the probability of each class for each value of the features\n",
    "prop_f_target_reduce = []\n",
    "for i in range(len(input_cols)):\n",
    "    prop_f_target_reduce.append(f_target_reduce[i].map(lambda x: (x[0][0], (x[0][1], x[1]))))\n",
    "    prop_f_target_reduce[i] = prop_f_target_reduce[i].join(f_reduce[i])\n",
    "    prop_f_target_reduce[i] = prop_f_target_reduce[i].map(lambda x: (x[0], (x[1][0][0], x[1][0][1]), x[1][1]))\n",
    "    prop_f_target_reduce[i] = prop_f_target_reduce[i].map(lambda x: (x[0], (x[1][0], x[1][1] / x[2])))\n",
    "    prop_f_target_reduce[i] = prop_f_target_reduce[i].groupByKey().mapValues(list)\n",
    "\n",
    "\n",
    "# Find number of records\n",
    "N = train_rdd_spark.count()\n",
    "prop_target_reduce = target_reduce.map(lambda x: (x[0], x[1] / N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the RDDs to dictionaries to use them in the prediction function\n",
    "prop_f_target_reduce_dict = []\n",
    "for i in range(len(input_cols)):\n",
    "    prop_f_target_reduce_dict.append(prop_f_target_reduce[i].collectAsMap())\n",
    "prop_target_reduce_dict = prop_target_reduce.collectAsMap()\n",
    "\n",
    "# Sort the values of the dictionaries by the class to use them in the prediction function\n",
    "for i in range(len(input_cols)):\n",
    "    for key in prop_f_target_reduce_dict[i]:\n",
    "        prop_f_target_reduce_dict[i][key].sort(key=lambda x: x[0])\n",
    "    \n",
    "prop_target_reduce_dict = sorted(prop_target_reduce_dict.items(), key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the target given features\n",
    "def predict(features):\n",
    "    f_target = []\n",
    "    for i in range(len(input_cols)):\n",
    "        if features[i]  in prop_f_target_reduce_dict[i]:\n",
    "            f_target.append(prop_f_target_reduce_dict[i][features[i]])\n",
    "            f_target[i] = [x[1] for x in f_target[i]]\n",
    "            if len(f_target[i]) < len(prop_target_reduce_dict):\n",
    "                if f_target[i][0] == 0:\n",
    "                    f_target[i].insert(1, 0)\n",
    "                else:\n",
    "                    f_target[i].insert(0, 0)\n",
    "        else:\n",
    "            f_target.append([0] * len(prop_target_reduce_dict))\n",
    "            \n",
    "    prob = [1] * len(f_target[0])\n",
    "\n",
    "    for j in range(len(f_target[0])):\n",
    "        for i in range(len(input_cols)):\n",
    "            prob[j] *= f_target[i][j]\n",
    "    # Argmax\n",
    "    prediction = prob.index(max(prob))\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier with MapReduce\n",
      "Accuracy: 77.70%\n",
      "Precision: 82.17%\n",
      "Recall: 77.70%\n",
      "F1: 68.11%\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "test_features = []\n",
    "for i in range(len(input_cols)):\n",
    "    test_features.append(test_data[input_cols[i]].tolist())\n",
    "\n",
    "y_true = test_data[output_col].tolist()\n",
    "y_pred = []\n",
    "\n",
    "for i in range(len(test_features[0])):\n",
    "    features = []\n",
    "    for j in range(len(input_cols)):\n",
    "        features.append(test_features[j][i])\n",
    "    prediction = predict(features)\n",
    "    y_pred.append(prediction)\n",
    "\n",
    "print(\"Naive Bayes Classifier with MapReduce\")\n",
    "print(f\"Accuracy: {accuracy_score(y_true, y_pred) * 100:.2f}%\")\n",
    "print(f\"Precision: {precision_score(y_true, y_pred, average='weighted') * 100:.2f}%\")\n",
    "print(f\"Recall: {recall_score(y_true, y_pred, average='weighted') * 100:.2f}%\")\n",
    "print(f\"F1: {f1_score(y_true, y_pred, average='weighted') * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
